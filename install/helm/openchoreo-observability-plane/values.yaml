## STRUCTURE OF THE VALUES FILE
# This values file has 2 main sections:
# 1. Values passed to dependent helm charts (like opentelemetry-collector, kgateway, etc)
# 2. Values passed to templates in the openchoreo-observability-plane chart itself


# Global values shared across all components
global:
  baseDomain: ""

  # Common labels to add to all resources
  commonLabels: {}

# Kubernetes cluster domain for controller manager defaults
kubernetesClusterDomain: cluster.local

controllerManager:
  enabled: true
  name: controller-manager
  replicas: 1
  image:
    repository: ghcr.io/openchoreo/controller
    tag: ""
    pullPolicy: IfNotPresent
  deploymentPlane: observabilityplane
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi
  serviceAccount:
    create: true
    annotations: {}
  priorityClass:
    create: false
    name: observabilityplane-controller-manager
    value: 900000
  nodeSelector: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: []
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: false
    seccompProfile:
      type: RuntimeDefault
  manager:
    args:
      - --metrics-bind-address=:0
      - --health-probe-bind-address=:8081
      - --deployment-plane=observabilityplane
    env:
      enableWebhooks: "false"
  clusterGateway:
    enabled: false
    url: https://cluster-gateway.openchoreo-control-plane.svc.cluster.local:8443
    tls:
      caPath: /etc/cluster-gateway/ca.crt
      caConfigMap: cluster-gateway-ca

  # Installation mode of OpenChoreo
  # Supported values:
  # singleCluster: When deploying all OpenChoreo planes in a single Kubernetes cluster
  # multiCluster: When deploying OpenChoreo planes in multiple Kubernetes clusters (To be supported in the future)
  # quickStart: Limited set of features for quick-start setup
  installationMode: singleCluster
  

## Values for dependent charts
data-prepper:
  enabled: false
  fullnameOverride: "data-prepper"

  pipelineConfig:
    enabled: true
    config:
      trace-pipeline:
        delay: "100"
        source:
          otel_trace_source:
            ssl: false
        buffer:
          bounded_blocking:
            buffer_size: 12800
            batch_size: 200
        sink:
          - opensearch:
              hosts: ["https://opensearch:9200"]
              password: ThisIsTheOpenSearchPassword1
              insecure: true
              index: otel-traces-%{yyyy-MM-dd}
              username: admin

  resources:
    limits:
      cpu: 1000m
      memory: 500Mi
    requests:
      cpu: 700m
      memory: 500Mi


# External Secrets Operator configuration for secret management
# Single cluster: Set to false to use data plane's ESO
# Multi-cluster: Set to true to install dedicated ESO in observability plane
external-secrets:
  enabled: false
  fullnameOverride: external-secrets
  nameOverride: external-secrets


# Fake Secret Store configuration for local development
fakeSecretStore:
  enabled: false
  name: default
  secrets:
    - key: RCA_LLM_API_KEY
      value: "fake-llm-api-key-for-development"

# kgateway configurations
kgateway:
  enabled: false
  fullnameOverride: "kgateway"

  # KGateway controller configuration
  controller:
    image:
      pullPolicy: IfNotPresent

    service:
      type: ClusterIP
      ports:
        agwGrpc: 9978

    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi


opentelemetry-collector:
  enabled: true
  clusterRole:
    create: true
    rules:
      - apiGroups: [""]
        resources: ["pods"]
        verbs: ["list", "watch"]
      - apiGroups: ["apps"]
        resources: ["replicasets"]
        verbs: ["list","watch"]

  configMap:
    create: false
    existingName: "opentelemetry-collector-config"

  fullnameOverride: "opentelemetry-collector"

  image:
    repository: otel/opentelemetry-collector-contrib

  mode: deployment

  resources:
    limits:
      cpu: 100m
      memory: 200Mi
    requests:
      cpu: 50m
      memory: 100Mi


openSearch:
  enabled: false

  extraEnvs:
    - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD
      value: ThisIsTheOpenSearchPassword1
  nameOverride: "opensearch"
  image:
    tag: "3.3.0"
  masterService: opensearch

  rbac:
    create: true
    serviceAccountName: "opensearch"

  singleNode: true

# OpenSearch configuration for operator based OpenSearch deployment
openSearchCluster:
  enabled: true

  bootstrap:
    resources:
      limits:
        cpu: 1000m
        memory: 1000Mi
      requests:
        cpu: 100m
        memory: 1000Mi

  general:
    setVMMaxMapCount: true
    version: 3.3.0

  dashboards:
    enable: false
    replicas: 1
    version: 3.3.0

  nodePools:
    data:
      replicas: 2
      diskSize: 5Gi
      resources:
        limits:
          cpu: 1000m
          memory: 1000Mi
        requests:
          cpu: 100m
          memory: 1000Mi
    master:
      replicas: 3
      diskSize: 1Gi
      resources:
        limits:
          cpu: 1000m
          memory: 900Mi
        requests:
          cpu: 100m
          memory: 900Mi

  # Secrets
  adminUsername: admin
  adminUserPassword: ThisIsTheOpenSearchPassword1
  internalUsers: |
    # This is the internal user database
    # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh

    _meta:
      type: "internalusers"
      config_version: 2

    admin:
      hash: "%s"
      reserved: true
      backend_roles:
      - "admin"
      description: "Admin user"

openSearchClusterSetup:
  image:
    repository: ghcr.io/openchoreo/init-observability-opensearch
    tag: ""
  observerAddress: "http://observer.openchoreo-observability-plane:8080"
  observerAlertingWebhookSecret: qxbfqk3yjiejrlelolvh # TODO: Move to a kubernetes secret

# Customizing OpenSearch Dashboards configurations
openSearchDashboards:
  enabled: false

  config:
    disableSecurity: "true"

  extraEnvs:
  - name: DISABLE_SECURITY_DASHBOARDS_PLUGIN
    value: "true"

  nameOverride: "opensearch-dashboards"
  fullnameOverride: "opensearch-dashboards"
  image:
    tag: "3.3.0"
  opensearchHosts: "http://opensearch:9200"
  replicas: 1

# OpenChoreo Observer Service Configuration
observer:
  replicas: 1

  extraEnvs:
  - name: OPENSEARCH_ADDRESS
    value: "https://opensearch:9200"
  - name: PROMETHEUS_ADDRESS
    value: "http://openchoreo-observability-prometheus:9091"
  - name: PROMETHEUS_TIMEOUT
    value: "30s"
  - name: ALERTING_WEBHOOK_SECRET
    value: "qxbfqk3yjiejrlelolvh" # TODO: Move to a kubernetes secret

  image:
    repository: ghcr.io/openchoreo/observer
    tag: ""  # If no value is set, use Chart.AppVersion
    pullPolicy: IfNotPresent

  service:
    type: ClusterIP
    port: 8080

  logLevel: info

  prometheus:
    # Prometheus server address (will be constructed from helm release name)
    address: ""  # If no value is set, use observability-plane-promet-prometheus:9090
    timeout: 30s

  resources:
    limits:
      cpu: 200m
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 128Mi

  openSearchUsername: admin
  openSearchPassword: ThisIsTheOpenSearchPassword1

# Fluent Bit configuration
fluentBit:
  enabled: false
  config:
    customParsers: |
      [PARSER]
          Name docker_no_time
          Format json
          Time_Keep Off
          Time_Key time
          Time_Format %Y-%m-%dT%H:%M:%S.%L

    inputs: |
      [INPUT]
          Name tail
          Buffer_Chunk_Size 32KB
          Buffer_Max_Size 2MB
          DB /var/lib/fluent-bit/db/tail-container-logs.db
          Exclude_Path /var/log/containers/fluent-bit-*_openchoreo-observability-plane_*.log
          Inotify_Watcher false
          Mem_Buf_Limit 256MB
          Path /var/log/containers/*.log
          multiline.parser docker, cri
          Read_from_Head On
          Refresh_Interval 5
          Skip_Long_Lines On
          Tag kube.*

    filters: |
      [FILTER]
          Name kubernetes
          Buffer_Size 32MB
          K8S-Logging.Parser On
          K8S-Logging.Exclude On
          Keep_Log On
          Match kube.*
          Merge_Log Off
          tls.verify Off
          Use_Kubelet true

    outputs: |
      [OUTPUT]
          Name opensearch
          Host opensearch
          Generate_ID On
          HTTP_Passwd admin
          HTTP_User admin
          Logstash_Format On
          Logstash_DateFormat %Y-%m-%d
          Logstash_Prefix container-logs
          Match kube.*
          Port 9200
          Replace_Dots On
          Suppress_Type_Name On
          tls On
          tls.verify Off

  dnsPolicy: ClusterFirstWithHostNet
  hostNetwork: true

  extraVolumeMounts:
  - name: db
    mountPath: "/var/lib/fluent-bit/db"
    readOnly: false
  extraVolumes:
    - name: db
      hostPath:
        path: /var/lib/fluent-bit/db
        type: DirectoryOrCreate

  fullnameOverride: "fluent-bit"
  metricsPort: 2021

  rbac:
    nodeAccess: true

  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi

  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 10000

  service:
    port: 2021

  testFramework:
    enabled: false

  initContainers:
  - name: set-volume-ownership
    image: busybox
    command: ["sh", "-c", "chown -R 10000:10000 /var/lib/fluent-bit/db"]
    resources:
      limits:
        cpu: 15m
        memory: "32Mi"
      requests:
        cpu: 10m
        memory: "24Mi"
    securityContext:
      capabilities:
        drop:
        - ALL
      privileged: true
      readOnlyRootFilesystem: true
      runAsUser: 0
    volumeMounts:
    - name: db
      mountPath: /var/lib/fluent-bit/db

# Prometheus configuration
prometheus:
  enabled: true
  fullnameOverride: openchoreo-observability

  ## Manages Prometheus and Alertmanager components
  prometheusOperator:
    enabled: true
    fullnameOverride: prometheus-operator

    resources:
      limits:
        cpu: 40m
        memory: 50Mi
      requests:
        cpu: 20m
        memory: 30Mi

  ## Install Prometheus Operator CRDs
  crds:
    enabled: true

  ## Setting to produces cleaner resource names
  cleanPrometheusOperatorObjectNames: true

  ## Deploy a Prometheus instance (Prometheus Operator needs to be enabled for this to work)
  prometheus:
    enabled: true # This section needs to be further configured for Thanos backups, priorityClass etc.
    prometheusSpec:
      serviceMonitorNamespaceSelector: {}
      serviceMonitorSelector: {}
      serviceMonitorSelectorNilUsesHelmValues: false

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  alertmanager:
    enabled: false
    alertmanagerSpec:
      podMetadata:
        name: alertmanager

  ## Configuration for grafana
  # Values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  grafana:
    enabled: false
    fullnameOverride: grafana
    adminUser: admin
    adminPassword: admin
    ## OpenChoreo Observability Plane does not ship default Grafana dashboards or datasources
    ## OpenChoreo specific dashboards and datasources will be added separately
    sidecar:
      dashboards:
        enabled: false
      datasources:
        enabled: false
    defaultDashboardsEnabled: false
    ## Datasources of OpenChoreo Observability Plane
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Prometheus
            type: prometheus
            access: proxy
            url: http://openchoreo-observability-prometheus:9091
            isDefault: true

  ## Flag to disable all the kubernetes component scrapers
  kubernetesServiceMonitors:
    enabled: true
  ## Component scraping kube state metrics
  kubeStateMetrics:
    enabled: true # kubeStateMetrics service monitor needs to be custom defined
  ## Configuration for kube-state-metrics subchart
  kube-state-metrics:
    fullnameOverride: kube-state-metrics
    collectors:
      - pods
      # Jobs and Services to be added later
    metricAllowlist:
      - kube_pod_completion_time
      - kube_pod_container_resource_limits
      - kube_pod_container_resource_requests
      - kube_pod_info
      - kube_pod_init_container_resource_limits
      - kube_pod_init_container_resource_requests
      - kube_pod_labels
      - kube_pod_start_time
      - kube_pod_status_phase
      # Jobs and Services to be added later
    metricLabelsAllowlist:
      - pods=[openchoreo.dev/component-uid,openchoreo.dev/project-uid,openchoreo.dev/environment-uid]
      # Jobs and Services to be added later

  ## Configuration for thanosRuler
  ## ref: https://thanos.io/tip/components/rule.md/
  thanosRuler:
    enabled: false # This section needs to be further configured for queryEndpoints etc.

  ## Create default rules for monitoring the cluster
  defaultRules:
    create: false

  ## Component scraping the kubelet and kubelet-hosted cAdvisor
  ## Disable when cAdvisor ServiceMonitor is added
  kubelet:
    enabled: true

  ## Openchoreo Observability is not meant for monitoring the kubernetes cluster
  ## Hence, following kube components are disabled
  kubeApiServer:
    enabled: false
  kubeControllerManager:
    enabled: false
  coreDns:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeProxy:
    enabled: false
  nodeExporter:
    enabled: false


## Values for local templates

# Cluster Agent configuration for agent-based communication with control plane
clusterAgent:
  enabled: true
  name: cluster-agent-observabilityplane
  replicas: 1
  image:
    repository: ghcr.io/openchoreo/cluster-agent
    tag: "" # If no value is set, use Chart.AppVersion
    pullPolicy: IfNotPresent
  # Server URL of the cluster gateway in control plane
  serverUrl: wss://cluster-gateway.openchoreo-control-plane.svc.cluster.local:8443/ws
  # Name of this observability plane
  planeName: default
  # Type of plane (dataplane, buildplane, or observabilityplane)
  planeType: observabilityplane
  # DNS rewrite configuration (for multi-cluster k3d setups)
  # When enabled, configures CoreDNS to rewrite *.openchoreo.localhost to host.k3d.internal
  dnsRewrite:
    enabled: false
  heartbeatInterval: 30s
  reconnectDelay: 5s
  logLevel: info
  resources:
    requests:
      cpu: "50m"
      memory: "128Mi"
    limits:
      cpu: "100m"
      memory: "256Mi"
  serviceAccount:
    create: true
    name: cluster-agent-observabilityplane
    annotations: {}
  rbac:
    create: true
  priorityClass:
    create: false
    name: cluster-agent-observabilityplane
    value: 900000
  tls:
    enabled: true
    # Generate client certificates locally using cert-manager (multi-cluster setup)
    # Set to true when the observability plane is in a different cluster from control plane
    # When true, cert-manager will generate the client certificates instead of copying CA from control plane
    generateCerts: false
    # Secret containing client certificate and key
    secretName: cluster-agent-tls
    clientSecretName: cluster-agent-tls
    # ConfigMap containing server CA certificate for verifying gateway
    serverCAConfigMap: cluster-gateway-ca
    # Optional: Inline server CA certificate value (PEM format)
    # For multi-cluster setup where agent needs to verify gateway server
    # If provided, this value is used instead of copying from control plane
    serverCAValue: ""
    # CA secret for signing agent client certificates
    # Option 1: Reference existing secret (single-cluster setup)
    # This should be the name of the cluster-gateway-ca secret created in control plane
    caSecretName: cluster-gateway-ca
    # Namespace where the CA secret exists (control plane namespace)
    caSecretNamespace: openchoreo-control-plane
    # Option 2: Inline CA certificate value (PEM format, multi-cluster setup)
    # If provided, this takes precedence over caSecretName/caSecretNamespace
    # Use this when observability plane is in a different cluster from control plane
    caValue: ""
    # Certificate duration and renewal
    duration: 2160h # 90 days
    renewBefore: 360h # 15 days
  # Namespace where cluster-gateway CA exists (control plane namespace)
  serverCANamespace: openchoreo-control-plane
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true
  podAnnotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}


gateway:
  enabled: false
  httpsPort: 443

# OpenSearch configuration for operator based OpenSearch deployment
openSearchCluster:
  enabled: true

  bootstrap:
    resources:
      limits:
        cpu: 1000m
        memory: 1000Mi
      requests:
        cpu: 100m
        memory: 1000Mi

  general:
    setVMMaxMapCount: true
    version: 3.3.0

  dashboards:
    enable: false
    replicas: 1
    version: 3.3.0

  nodePools:
    data:
      replicas: 2
      diskSize: 5Gi
      resources:
        limits:
          cpu: 1000m
          memory: 1000Mi
        requests:
          cpu: 100m
          memory: 1000Mi
    master:
      replicas: 3
      diskSize: 1Gi
      resources:
        limits:
          cpu: 1000m
          memory: 900Mi
        requests:
          cpu: 100m
          memory: 900Mi

  # Secrets
  adminUsername: admin
  adminUserPassword: ThisIsTheOpenSearchPassword1
  internalUsers: |
    # This is the internal user database
    # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh

    _meta:
      type: "internalusers"
      config_version: 2

    admin:
      hash: "%s"
      reserved: true
      backend_roles:
      - "admin"
      description: "Admin user"

openSearchClusterSetup:
  image:
    repository: ghcr.io/openchoreo/init-observability-opensearch
    tag: ""


## opentelemetryCollectorCustomizations aren't passed to the opentelemetry-collector helm values file directly.
# They are to be used by openchoreo specific templates
opentelemetryCollectorCustomizations:
  openSearchQueue:
    numConsumers: 5
    queueSize: 1000
    sizer: items
  tailSampling:
    decisionWait: 10s
    numTraces: 100
    expectedNewTracesPerSec: 10
    decisionCache:
      sampledCacheSize: 10000
      nonSampledCacheSize: 1000
    spansPerSecond: 10

tls:
  enabled: false

# Wait job configuration for post-install hooks
waitJob:
  image: bitnamilegacy/kubectl:1.32.4
